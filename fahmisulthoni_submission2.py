# -*- coding: utf-8 -*-
"""FahmiSulthoni_Submission2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16bnDXj68meP6HIdwnSpBgMYYOTL4HtYj

*   Nama: Fahmi Sulthoni
*   Email: fahmi.sulthoni30@gmail.com
"""

import numpy as np
import pandas as pd
from keras.layers import Dense, LSTM
import matplotlib.pyplot as plt
import tensorflow as tf
from sklearn.preprocessing import MinMaxScaler
import matplotlib.pyplot as plt

datasetnya = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/00374/energydata_complete.csv')
datasetnya

tanggal = datasetnya['date'].values
energi  = datasetnya['Appliances'].values
     
plt.figure(figsize=(15,5))
plt.plot(tanggal, energi)
plt.title('Energi yang Digunakan',
              fontsize=20);

datalatihnya = int(len(energi) * 0.8)
datatesnya = len(energi) - datalatihnya
datatrain, datatest = energi[0:datalatihnya], energi[datatesnya:len(energi)]
print(len(datatrain),len(datatest))

min = min(datatrain)
max = max(datatest)
print(min,max)
lower = (max - min) / 10
print(lower)

def windowed_dataset(series, window_size, batch_size, shuffle_buffer):
    series = tf.expand_dims(series, axis=-1)
    ds = tf.data.Dataset.from_tensor_slices(series)
    ds = ds.window(window_size + 1, shift=1, drop_remainder=True)
    ds = ds.flat_map(lambda w: w.batch(window_size + 1))
    ds = ds.shuffle(shuffle_buffer)
    ds = ds.map(lambda w: (w[:-1], w[1:]))
    return ds.batch(batch_size).prefetch(1)

datatrain_set = windowed_dataset(datatrain.astype('float32'), window_size=60, batch_size=100, shuffle_buffer=1000)
datatrain_set

datatest_set = windowed_dataset(datatest.astype('float32'), window_size=60, batch_size=100, shuffle_buffer=1000)
datatest_set

model = tf.keras.models.Sequential([
  tf.keras.layers.LSTM(60, return_sequences=True),
  tf.keras.layers.LSTM(60),
  tf.keras.layers.Dense(30, activation="relu"),
  tf.keras.layers.Dense(10, activation="relu"),
  tf.keras.layers.Dense(1),
])

class myCallback(tf.keras.callbacks.Callback):
    def on_epoch_end(self, epoch, logs={}):
      if(logs.get('mae') < 45.0):
        print("\nmae sudah kurang dari 44.5!")
        self.model.stop_training = True
callbacks = myCallback()

optimizer = tf.keras.optimizers.SGD(lr=1.0000e-04, momentum=0.9)

model.compile(loss=tf.keras.losses.Huber(),
              optimizer=optimizer,
              metrics=["mae"])
history = model.fit(datatrain_set, callbacks=[callbacks], validation_data=datatest_set, epochs=100)

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'])
plt.show()

plt.plot(history.history['mae'])
plt.plot(history.history['val_mae'])
plt.title('Model mae')
plt.ylabel('MAE')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='lower right')
plt.show()